STATUS: DOING

# PR 001: The Harvester

**Objective:** Implement the automatic data fetching layer to source papers from arXiv.

**Context:**
This is the foundational layer of specific data ingestion. We need a reliable way to query arXiv for specific categories relevant to our domain (Math, CS, Physics, Quant Bio) and standardize the data.

**Requirements:**

1.  **Dependency Management:**
    *   Create `requirements.txt`.
    *   Add `arxiv` (official python wrapper) and `requests`.

2.  **Core Logic (`harvester.py`):**
    *   Create a function `fetch_papers(categories: list[str], max_results: int = 20) -> list[dict]`.
    *   **Input:** List of category strings (e.g., `['cs.CL', 'cs.LG', 'stat.ML']`).
    *   **Output:** structured list of dictionaries, where each dict contains:
        *   `arxiv_id`: Unique ID (e.g., `2310.12345`).
        *   `title`: String.
        *   `abstract`: String (clean up newlines if necessary).
        *   `authors`: List of strings.
        *   `url`: Direct link to the abstract page.
        *   `published`: Datetime object.
        *   `categories`: List of categories the paper belongs to.

3.  **Verification:**
    *   Include a `if __name__ == "__main__":` block.
    *   In this block, call the function for category `cs.AI` and print the titles of the first 5 returned papers to console to prove connectivity.

**Acceptance Criteria:**
*   Script runs without errors.
*   Correctly fetches real data from arXiv.
*   Data structure matches the requirement.
