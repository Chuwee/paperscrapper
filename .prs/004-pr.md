STATUS: DOING

# PR 004: The Brain (LLM & Embeddings)

**Objective:** Implement the recommendation intelligence using Embeddings for history and LLM for filtering.

**Context:**
This is the "AI" part. We need to screen new papers against what we know the user likes. Using a simple Vector comparison for broad strokes + an LLM for the final decision is a cost-effective 2-step pipeline.

**Requirements:**

1.  **Dependencies:**
    *   Add `openai` (or `anthropic`) to `requirements.txt`.
    *   Add `chromadb` or `numpy` (if using simple cosine similarity). Let's stick to `numpy` + `scikit-learn` for simplicity if the dataset is small, or `chromadb` if we want persistence. **Decision:** Use `chromadb` for persistent vector storage.

2.  **Logic (`brain.py`):**
    *   **`class RecommendationEngine`**:
        *   `__init__`: Connect to ChromaDB.
        *   `update_user_profile(user_id)`:
            *   Fetch all "Interested" papers for this user from SQLite.
            *   Generate embeddings for their abstracts (use a small model like `all-MiniLM-L6-v2` via `sentence_transformers` or an API).
            *   Store/Average these embeddings to create a "User Vector".
        *   `evaluate_paper(user_id, paper)`:
            *   **Step 1 (Fast Filter):** Compare paper embedding to User Vector. If similarity > 0.3 (threshold), proceed. Else return `False`.
            *   **Step 2 (LLM Judge):** If pass Step 1, call LLM.
    *   **LLM Prompt Design**:
        *   Input: User's top 3 liked paper titles + Current Paper Abstract.
        *   System Prompt: "You constitute a research assistant. Based on the user's history, decide if this new paper is relevant. Reply ONLY with boolean TRUE or FALSE."
    
3.  **Functions:**
    *   `get_paper_score(user_id, paper_text) -> bool`

**Acceptance Criteria:**
*   Can generate an embedding for a text string.
*   Can store/retrieve user vectors.
*   `evaluate_paper` returns a boolean.
*   Mock the LLM call for testing to save tokens.
